{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mplsoccer import Sbopen\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import streamlit as st\n",
    "from joblib import load\n",
    "from db_connection import get_db\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sql_schemas import PlayingTimes, Competitions\n",
    "from db_connection import get_db\n",
    "from sqlalchemy import text\n",
    "import seaborn as sns\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Pitch Zones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pitch_zone(x, y):\n",
    "    # Define horizontal zones (x-axis)\n",
    "    if x < 40:\n",
    "        horiz_zone = 'Defensive'\n",
    "    elif x <= 80:\n",
    "        horiz_zone = 'Middle'\n",
    "    else:\n",
    "        horiz_zone = 'Attacking'\n",
    "\n",
    "    # Define vertical zones (y-axis)\n",
    "    if y < 20:\n",
    "        vert_zone = 'Right'\n",
    "    elif y <= 60:\n",
    "        vert_zone = 'Center'\n",
    "    else:\n",
    "        vert_zone = 'Left'\n",
    "    \n",
    "    return f\"{horiz_zone} {vert_zone}\"\n",
    "\n",
    "# Apply function to create start and end zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event = pd.read_csv('../../excel/event_df.csv')\n",
    "track_df = pd.read_csv('../../excel/track_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event = df_event.dropna(subset=['x', 'y'])\n",
    "df_event = df_event[~df_event[['x', 'y']].isin([float('inf'), float('-inf')]).any(axis=1)]\n",
    "df_event.loc[df_event['end_x'].isna(), 'end_x'] = df_event['x']\n",
    "df_event.loc[df_event['end_y'].isna(), 'end_y'] = df_event['y']\n",
    "\n",
    "df_event['start_zone'] = df_event.apply(lambda row: get_pitch_zone(row['x'], row['y']), axis=1)\n",
    "df_event['end_zone'] = df_event.apply(lambda row: get_pitch_zone(row['end_x'], row['end_y']), axis=1)\n",
    "\n",
    "df_event[\"position_name\"].unique()\n",
    "\n",
    "position_map = {\n",
    "\n",
    "    \"Center Forward\": \"Striker\",\n",
    "    \"Left Center Forward\": \"Striker\",\n",
    "    \"Right Center Forward\": \"Striker\",\n",
    "    \"Second Striker\": \"Striker\",\n",
    "    \"Striker\": \"Striker\",\n",
    "\n",
    "    # --- Center Backs ---\n",
    "    \"Center Back\": \"Center Back\",\n",
    "    \"Left Center Back\": \"Center Back\",\n",
    "    \"Right Center Back\": \"Center Back\",\n",
    "\n",
    "    # --- Fullbacks & Wingbacks ---\n",
    "    \"Left Back\": \"Wide Back\",\n",
    "    \"Right Back\": \"Wide Back\",\n",
    "    \"Left Wing Back\": \"Wide Back\",\n",
    "    \"Right Wing Back\": \"Wide Back\",\n",
    "\n",
    "    # --- Central Midfielders ---\n",
    "    \"Center Midfield\": \"Center Midfield\",\n",
    "    \"Left Center Midfield\": \"Center Midfield\",\n",
    "    \"Right Center Midfield\": \"Center Midfield\",\n",
    "    \"Center Defensive Midfield\": \"Center Midfield\",\n",
    "    \"Left Defensive Midfield\": \"Center Midfield\",\n",
    "    \"Right Defensive Midfield\": \"Center Midfield\",\n",
    "    \"Center Attacking Midfield\": \"Center Midfield\",\n",
    "    \"Left Attacking Midfield\": \"Center Midfield\",\n",
    "    \"Right Attacking Midfield\": \"Center Midfield\",\n",
    "\n",
    "    # --- Wide Midfielders / Wingers ---\n",
    "    \"Left Midfield\": \"Wide Midfield\",\n",
    "    \"Right Midfield\": \"Wide Midfield\",\n",
    "    \"Left Wing\": \"Wide Midfield\",\n",
    "    \"Right Wing\": \"Wide Midfield\",\n",
    "\n",
    "    \"Goalkeeper\": \"Goalkeeper\",\n",
    "}\n",
    "\n",
    "df_event[\"pos_group\"] = df_event[\"position_name\"].map(position_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Compute Movement Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_event['distance'] = np.sqrt((df_event['x'] - df_event['end_x']) ** 2 + (df_event['y'] - df_event['end_y']) ** 2)\n",
    "df_event['progression'] = df_event['end_x'] - df_event['x']\n",
    "df_event['directness'] = np.where(df_event['distance'] != 0, df_event['progression'] / df_event['distance'], 0)\n",
    "df_event['width_change'] = abs(df_event['end_y'] - df_event['y'])\n",
    "#df_event['speed'] = np.where(df_event['duration'] != 0, df_event['distance'] / df_event['duration'], 0)\n",
    "#df_event['distance_covered'] = np.sqrt((df_event['end_x'] - df_event['x']) ** 2 + (df_event['end_y'] - df_event['y']) ** 2)\n",
    "#df_event['angle_change'] = np.arctan2(df_event['end_y'] - df_event['y'], df_event['end_x'] - df_event['x'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Packing Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_pos = df_event.loc[:,['id', 'x', 'y', 'end_x', 'end_y']]\n",
    "id_pos = id_pos.rename(columns={'x': 'ball_x', 'y': 'ball_y'})\n",
    "track_df_full = track_df.merge(id_pos, on='id', how='left')\n",
    "track_df_full = track_df_full[track_df_full['actor']==False]\n",
    "track_df_full['distance'] = np.sqrt((track_df_full['x'] - track_df_full['ball_x'])**2 + (track_df_full['y'] - track_df_full['ball_y'])**2)\n",
    "\n",
    "# ball went behind the opponent\n",
    "track_df_full['packing'] = (track_df_full['x'] > track_df_full['ball_x']) & (track_df_full['x'] < track_df_full['end_x'])\n",
    "\n",
    "\n",
    "teammates_df = track_df_full[track_df_full['teammate'] == True]\n",
    "teammates_df = teammates_df.rename(columns={'distance': 'teammate_distance'})\n",
    "\n",
    "teammates_df['teammate_behind_ball'] = (teammates_df['x'] > teammates_df['ball_x'])\n",
    "\n",
    "opponents_df = track_df_full[track_df_full['teammate'] == False]\n",
    "opponents_df = opponents_df.rename(columns={'distance': 'opponent_distance'})\n",
    "\n",
    "track_df_full['nearby_opponents'] = (track_df_full['distance'] < 10) & (track_df_full['teammate']==False)\n",
    "track_df_full['nearby_teammates'] = (track_df_full['distance'] < 10) & (track_df_full['teammate']==True)\n",
    "\n",
    "\n",
    "teammate_distance_min = teammates_df.groupby(['id'])['teammate_distance'].min().reset_index()\n",
    "opponent_distance_min = opponents_df.groupby('id')['opponent_distance'].min().reset_index()\n",
    "\n",
    "distance_count = track_df_full.groupby('id')['nearby_opponents'].sum().reset_index()\n",
    "distance_count_teammates = track_df_full.groupby('id')['nearby_teammates'].sum().reset_index()\n",
    "packing = opponents_df.groupby('id')['packing'].sum().reset_index()\n",
    "teammate_forward = teammates_df.groupby('id')['teammate_behind_ball'].sum().reset_index()\n",
    "\n",
    "\n",
    "df_event = df_event.merge(distance_count, on='id', how='left')\n",
    "df_event = df_event.merge(distance_count_teammates, on='id', how='left')\n",
    "df_event = df_event.merge(teammate_distance_min, on='id', how='left')\n",
    "df_event = df_event.merge(opponent_distance_min, on='id', how='left')\n",
    "df_event = df_event.merge(packing, on='id', how='left')\n",
    "df_event = df_event.merge(teammate_forward, on='id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_heatmap(x, y, bins=(12,4), xrange=(0, 120), yrange=(0, 40)):\n",
    "    heatmap, _, _ = np.histogram2d(\n",
    "        x, y,\n",
    "        bins=bins,\n",
    "        range=[xrange, yrange]\n",
    "    )\n",
    "    heatmap = heatmap / np.max(heatmap) if np.max(heatmap) > 0 else heatmap\n",
    "    return heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_name = pd.read_excel('../../excel/player_features.xlsx').loc[:,[\"player_id\", \"player_name\", \"minutes\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = df_event.groupby('player_id')\n",
    "\n",
    "heatmaps = []\n",
    "ids = []\n",
    "\n",
    "for player_id, group in grouped:\n",
    "    # Make heatmap for this player\n",
    "    h = make_heatmap(group[\"x\"], abs(40-group[\"y\"]))\n",
    "    \n",
    "    # Scale to 0-1 for this player\n",
    "    h_min, h_max = h.min(), h.max()\n",
    "    if h_max > h_min:  # avoid division by zero\n",
    "        h = (h - h_min) / (h_max - h_min)\n",
    "    else:\n",
    "        h = np.zeros_like(h)  # if all values are the same\n",
    "    if player_name.loc[player_name[\"player_id\"] == player_id, \"minutes\"].values[0] > 500:\n",
    "        heatmaps.append(h)\n",
    "        ids.append(player_id)\n",
    "\n",
    "\n",
    "normalized_heatmaps = []  # shape: (N, H, W)\n",
    "\n",
    "\n",
    "for h in heatmaps:  # heatmaps.shape = (N, H, W)\n",
    "    h_min, h_max = h.min(), h.max()\n",
    "    if h_max > h_min:\n",
    "        h_norm = (h - h_min) / (h_max - h_min)  # scale 0-1\n",
    "    else:\n",
    "        h_norm = np.zeros_like(h)\n",
    "    normalized_heatmaps.append(h_norm)\n",
    "\n",
    "normalized_heatmaps = np.stack(normalized_heatmaps)\n",
    "\n",
    "X = torch.tensor(heatmaps).unsqueeze(1).float()  # (N, 1, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of players to visualize\n",
    "def visualize_players(heatmaps, ids, file_name = \"\"):   \n",
    "    num_players = 10\n",
    "    rows, cols = 2, 5\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(12, 12))\n",
    "    for i in range(num_players):\n",
    "        r, c = divmod(i, cols)\n",
    "        ax = axes[r, c]\n",
    "        ax.imshow(\n",
    "            heatmaps[i],\n",
    "            cmap='Greens',\n",
    "            origin='lower'\n",
    "        )\n",
    "        \n",
    "        # Get player name safely\n",
    "        match = player_name[player_name[\"player_id\"] == ids[i]]\n",
    "        if not match.empty:\n",
    "            name = match.iloc[0].player_name\n",
    "        else:\n",
    "            name = \"Unknown\"\n",
    "        \n",
    "        ax.set_title(name)\n",
    "        ax.set_xlabel(\"Width from center (10m)\")\n",
    "        ax.set_ylabel(\"Depth from goalline (10m)\")\n",
    "\n",
    "    if (file_name == \"ret\"):\n",
    "        plt.close(fig)\n",
    "        return fig\n",
    "    if (file_name != \"\"):\n",
    "        plt.savefig(file_name)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "visualize_players(heatmaps,ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BottleneckAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dim=8):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(48, 32),  # input 12*4=48 → hidden 32\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, latent_dim),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 48),\n",
    "            nn.Sigmoid()  # if inputs normalized to [0,1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten from [B, 1, 12, 4] → [B, 48]\n",
    "        x = x.view(x.size(0), -1)\n",
    "        z = self.encoder(x)\n",
    "        x = self.decoder(z)\n",
    "        # Reshape back to [B, 1, 12, 4]\n",
    "        return x.view(x.size(0), 1, 12, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X)\n",
    "dataloader = DataLoader(dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)\n",
    "model = BottleneckAutoencoder(latent_dim=12).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(50):\n",
    "    for batch in dataloader:\n",
    "        inputs = batch[0].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch+1\n",
    "print(f\"Epoch {epoch}, Loss: {loss.item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    recon = model(inputs).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.random.choice(len(X), size=5, replace=False)\n",
    "\n",
    "# Get reconstructions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    inputs = X[indices].to(device)\n",
    "    recons = model(inputs).cpu()\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 12))\n",
    "\n",
    "for i in range(5):\n",
    "    row = 0 if i < 5 else 1   # first 5 originals in row 0, next 5 reconstructions in row 1\n",
    "    col = i % 5\n",
    "\n",
    "    axes[0, col].imshow(inputs[i,0].cpu(), cmap='Greens', origin='lower')\n",
    "    axes[0, col].set_title(f\"{player_name[player_name[\"player_id\"] == ids[indices[i]]].iloc[0].player_name}\\nOriginal\")\n",
    "    axes[0, col].set_xlabel(\"Width from center (10m)\")\n",
    "    axes[0, col].set_ylabel(\"Depth from goalline (10m)\")\n",
    "\n",
    "\n",
    "    axes[1, col].imshow(recons[i,0], cmap='Greens', origin='lower')\n",
    "    axes[1, col].set_title(f\"Reconstruction\")\n",
    "    axes[1, col].set_xlabel(\"Width from center (10m)\")\n",
    "    axes[1, col].set_ylabel(\"Depth from goalline (10m)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Player level aggregation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\", \".join(df_event.columns.to_list()))\n",
    "df_event['sub_type_name'].unique()\n",
    "df_event['body_part_name'].unique()\n",
    "df_event[df_event['type_name'] == \"Dribble\"]['outcome_name'].unique()\n",
    "#df_event[df_event['type_name'] == \"Shot\"]['shot_first_time'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pressure = df_event.groupby(['player_id'])['under_pressure'].sum().reset_index()\n",
    "df_pressure.rename(columns={'under_pressure': 'pressure_count'}, inplace=True)\n",
    "\n",
    "# Count event types \n",
    "df_counts = df_event.groupby(['player_id'])['type_name'].value_counts().unstack(fill_value=0).reset_index()\n",
    "df_counts = df_counts[['player_id', 'Pass', 'Carry', 'Dribble', 'Shot', 'Duel', 'Pressure', 'Block', 'Ball Recovery', 'Interception', 'Clearance', 'Foul Won', 'Foul Committed', 'Goal Keeper']].fillna(0)\n",
    "df_counts.rename({\"Goal Keeper\": \"gk_action\"})\n",
    "df_position_groups = df_event.groupby(['player_id'])['pos_group'].value_counts(normalize=True).unstack(fill_value=0).reset_index()\n",
    "\n",
    "# Passes\n",
    "\n",
    "df_event['forward'] = df_event[\"end_x\"] - df_event[\"x\"]\n",
    "df_event['vertical'] = abs(df_event[\"y\"] - df_event[\"end_y\"])\n",
    "df_event['width'] = abs(40 - df_event[\"y\"])\n",
    "\n",
    "df_pass = df_event[(df_event['type_name'] == 'Pass')].groupby(['player_id']).agg(\n",
    "    all_passes = ('pass_length', 'count'),\n",
    "    successful_passes = ('outcome_name', lambda x: (x.isin([np.nan, 'Complete', 'Success', 'Success In Play', 'Success To Team'])).sum()),\n",
    "    avg_pass_length = ('pass_length', 'mean'),\n",
    "    avg_pass_angle = ('pass_angle', 'mean'),\n",
    "    median_pass_length = ('pass_length', 'median'),\n",
    "    longest_forward_pass = ('progression', 'max'),\n",
    "    pass_switch = ('pass_switch', 'count'),\n",
    "    pass_cut_back = ('pass_cut_back', 'count'),\n",
    "    assists = ('pass_goal_assist', 'count'),\n",
    "    pass_median_start = ('x', 'median'),\n",
    "    pass_median_end = ('end_x', 'median'),\n",
    "    pass_median_width_start = ('width', 'median'),\n",
    "    pass_median_width_end = ('width', 'median'),\n",
    ").reset_index()\n",
    "df_pass['pass_success_rate'] = df_pass['successful_passes'] / df_pass['all_passes']\n",
    "df_pass.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df_pass_types = df_event[df_event['type_name'] == 'Pass'].groupby(['player_id'])['pass_height_name'].value_counts(normalize=True).unstack(fill_value=0).reset_index()\n",
    "df_pass_types.columns = ['player_id'] + [f'{col.lower().replace(\" \", \"_\")}' for col in df_pass_types.columns[1:]]\n",
    "\n",
    "# Shots\n",
    "\n",
    "df_shot = df_event[(df_event['type_name'] == 'Shot')].groupby(['player_id']).agg(\n",
    "    all_shots = ('id', 'count'),\n",
    "    shot_mean_length = ('x', 'mean'),\n",
    "    shot_median_length = ('x', 'median'),\n",
    "    headers = ('body_part_name', lambda x: (x.isin(['Head'])).sum()),\n",
    "    extra_shots = ('technique_name', lambda x: (~x.isin(['Normal'])).sum()),\n",
    "    first_time_shots = ('shot_first_time', 'count'),\n",
    "    total_xg = ('shot_statsbomb_xg', 'sum'),\n",
    "    goals = ('outcome_name', lambda x: (x.isin(['Goal'])).sum()),\n",
    "    shots_on_target = ('outcome_name', lambda x: x.isin(['Goal', 'Saved']).sum())\n",
    ")\n",
    "df_shot['headers'] = df_shot['headers'] / df_shot['all_shots']\n",
    "df_shot['extra_shots'] = df_shot['extra_shots'] / df_shot['all_shots']\n",
    "df_shot['first_time_shots'] = df_shot['first_time_shots'] / df_shot['all_shots']\n",
    "\n",
    "df_shot['shot_mean_length'] = df_shot['shot_mean_length'].fillna(\n",
    "    df_shot['shot_mean_length'].median()\n",
    ")\n",
    "df_shot['shot_median_length'] = df_shot['shot_median_length'].fillna(\n",
    "    df_shot['shot_median_length'].median()\n",
    ")\n",
    "df_shot.fillna(0, inplace=True)\n",
    "\n",
    "# Dribbles\n",
    "df_dribble = df_event[df_event['type_name'] == 'Dribble'].groupby(['player_id']).agg(\n",
    "    all_dribbles = ('id', 'count'),\n",
    "    successful_dribbles_count = ('outcome_name', lambda x: (x.isin(['Complete'])).sum()),\n",
    "    dribble_start_x = ('x', 'mean')\n",
    ")\n",
    "df_dribble['successful_dribbles'] = df_dribble['successful_dribbles_count'] / df_dribble['all_dribbles']\n",
    "\n",
    "df_dribble.fillna(0, inplace=True)\n",
    "\n",
    "print(df_event['forward'].max())\n",
    "print(df_event[df_event['type_name']=='Carry']['end_x'].describe())\n",
    "print(df_event[df_event['type_name']=='Carry']['forward'].describe())\n",
    "\n",
    "df_carry = df_event[df_event['type_name'] == 'Carry'].groupby(['player_id']).agg(\n",
    "    carry_length_forward = ('forward', 'mean'),\n",
    "    median_carry_length_forward =  ('forward', 'median'),\n",
    "    carry_length_vertical =  ('vertical', 'mean'),\n",
    "    median_carry_start = ('x', 'median'),\n",
    "    median_carry_width = ('width', 'median'),\n",
    "    median_carry_end = ('end_x', 'median')\n",
    ")\n",
    "df_carry.fillna(0, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# Aggregate movement features\n",
    "df_movement = df_event.groupby(['player_id']).agg(\n",
    "    total_distance=('distance', 'sum'),\n",
    "    avg_progression=('progression', 'mean'),\n",
    "    avg_directness=('directness', 'mean'),\n",
    "    total_width_change=('width_change', 'sum'),\n",
    ").reset_index()\n",
    "\n",
    "# Aggregate pitch zone counts (start zones)\n",
    "df_pitch_zones = df_event.groupby(['player_id'])['start_zone'].value_counts(normalize=True).unstack(fill_value=0).reset_index()\n",
    "df_pitch_zones.columns = ['player_id'] + [f'start_{col.lower().replace(\" \", \"_\")}' for col in df_pitch_zones.columns[1:]]\n",
    "\n",
    "# Aggregate pitch zone counts (end zones)\n",
    "df_pitch_zones_end = df_event.groupby(['player_id'])['end_zone'].value_counts(normalize=True).unstack(fill_value=0).reset_index()\n",
    "df_pitch_zones_end.columns = ['player_id'] + [f'end_{col.lower().replace(\" \", \"_\")}' for col in df_pitch_zones_end.columns[1:]]\n",
    "\n",
    "\n",
    "# Pressure & support\n",
    "df_pressure_support = df_event.groupby(['player_id']).agg(\n",
    "    avg_opponent_distance = ('opponent_distance', 'mean'),\n",
    "    avg_teammate_distance = ('teammate_distance', 'mean'),\n",
    "    avg_opponent_count = ('nearby_opponents', 'mean'),\n",
    "    avg_teammate_count = ('nearby_teammates', 'mean'),\n",
    "    avg_advanced_players = ('teammate_behind_ball', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "# Packing\n",
    "df_packing = df_event.groupby(['player_id']).agg(\n",
    "    total_packing=('packing', 'sum')).reset_index()\n",
    "\n",
    "\n",
    "df_counterpress = df_event.groupby(['player_id'])['counterpress'].sum().reset_index()\n",
    "df_counterpress.rename(columns={'counterpress': 'counterpress_count'}, inplace=True)\n",
    "\n",
    "\n",
    "df_aerial_duels = df_event.groupby(['player_id'])['aerial_won'].sum().reset_index()\n",
    "df_aerial_duels.rename(columns={'aerial_won': 'aerial_duels_won'}, inplace=True)\n",
    "\n",
    "df_duration = df_event.groupby(['player_id']).agg(avg_duration=('duration', 'mean')).reset_index()\n",
    "\n",
    "\n",
    "# Merge all aggregated data into a possession-level DataFrame\n",
    "player_features = df_counts \\\n",
    "    .merge(df_pass, on=['player_id'], how='left') \\\n",
    "    .merge(df_shot, on=['player_id'], how='left') \\\n",
    "    .merge(df_dribble, on=['player_id'], how='left') \\\n",
    "    .merge(df_carry, on=['player_id'], how='left') \\\n",
    "    .merge(df_pressure, on=['player_id'], how='left') \\\n",
    "    .merge(df_movement, on=['player_id'], how='left') \\\n",
    "    .merge(df_pass_types, on=['player_id'], how='left') \\\n",
    "    .merge(df_position_groups, on=['player_id'], how='left') \\\n",
    "    .merge(df_pitch_zones, on=['player_id'], how='left') \\\n",
    "    .merge(df_pitch_zones_end, on=['player_id'], how='left') \\\n",
    "    .merge(df_counterpress[['player_id', 'counterpress_count']], on=['player_id'], how='left') \\\n",
    "    .merge(df_aerial_duels[['player_id', 'aerial_duels_won']], on=['player_id'], how='left') \\\n",
    "    .merge(df_duration[['player_id', 'avg_duration']], on=['player_id'], how='left') \\\n",
    "    .merge(df_pressure_support, on=['player_id'], how='left') \\\n",
    "    .merge(df_packing, on=['player_id'], how='left')\n",
    "\n",
    "# Fill NaN values with 0 where needed\n",
    "player_features.fillna(0, inplace=True)\n",
    "possession_features_original = player_features.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_convert = [ 'pass_switch', 'pass_cut_back', 'aerial_duels_won']\n",
    "\n",
    "for col in columns_to_convert:\n",
    "    player_features[col] = player_features[col].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_features.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = next(get_db())\n",
    "playing_times = db.query(PlayingTimes).all()\n",
    "db.close()\n",
    "df = pd.DataFrame([vars(r) for r in playing_times]).drop(columns=\"_sa_instance_state\")\n",
    "df\n",
    "total_minutes_df = df.groupby([\"player_id\"])[\"minutes\"].sum().reset_index()\n",
    "total_minutes_df\n",
    "names_df = df.groupby([\"player_id\"])[\"player_name\"].min().reset_index()\n",
    "player_features = player_features.merge(total_minutes_df, on=['player_id'], how='left')\n",
    "player_features = player_features.merge(names_df, on=['player_id'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_features.to_excel('../../excel/player_features.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_features = pd.read_excel('../../excel/player_features.xlsx')\n",
    "player_features = player_features[player_features['minutes'] > 500]\n",
    "player_features.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wide Focus = Width-related features (Summing all start & end width-related positions)\n",
    "player_features[\"wide_focus\"] = (\n",
    "    player_features[\"start_attacking_left\"] +\n",
    "    player_features[\"start_attacking_right\"] +\n",
    "    player_features[\"start_middle_left\"] +\n",
    "    player_features[\"start_middle_right\"] +\n",
    "    player_features[\"start_defensive_left\"] +\n",
    "    player_features[\"start_defensive_right\"] +\n",
    "    player_features[\"end_attacking_left\"] +\n",
    "    player_features[\"end_attacking_right\"] +\n",
    "    player_features[\"end_middle_left\"] +\n",
    "    player_features[\"end_middle_right\"] +\n",
    "    player_features[\"end_defensive_left\"] +\n",
    "    player_features[\"end_defensive_right\"]\n",
    ")\n",
    "\n",
    "player_features[\"defensive_start\"] = (\n",
    "    player_features[\"start_defensive_left\"] +\n",
    "    player_features[\"start_defensive_right\"] +\n",
    "    player_features[\"start_defensive_center\"]\n",
    ")\n",
    "player_features[\"middle_start\"] = (\n",
    "    player_features[\"start_middle_left\"] +\n",
    "    player_features[\"start_middle_right\"] +\n",
    "    player_features[\"start_middle_center\"]\n",
    ")\n",
    "player_features[\"attacking_start\"] = (\n",
    "    player_features[\"start_attacking_left\"] +\n",
    "    player_features[\"start_attacking_right\"] +\n",
    "    player_features[\"start_attacking_center\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "per90_columns = ['Pass', 'Carry', 'Dribble', 'Shot', 'Duel', 'Pressure',\n",
    "       'Block', 'Ball Recovery', 'Interception', 'Clearance', 'Foul Won',\n",
    "       'Foul Committed', 'pass_switch', 'pass_cut_back','total_xg',\n",
    "       'successful_dribbles_count', 'pressure_count',\n",
    "       'total_distance', 'total_width_change', 'counterpress_count',\n",
    "       'aerial_duels_won', 'total_packing', 'wide_focus',\n",
    "       'shots_on_target','assists','goals']\n",
    "player_features[per90_columns] = player_features[per90_columns].div(player_features[\"minutes\"], axis=0) * 90\n",
    "\n",
    "player_features.to_excel('../../excel/player_features_p90.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dropping original columns to avoid redundancy\n",
    "features_to_drop = [\n",
    "    # Wide Focus Components\n",
    "    \"start_attacking_left\", \"start_attacking_right\", \"start_middle_left\", \"start_middle_right\", \"start_defensive_left\", \"start_defensive_right\",\n",
    "    \"start_attacking_center\", \"start_defensive_center\", \"start_middle_center\",\n",
    "    \"end_attacking_left\", \"end_attacking_right\", \"end_middle_left\", \"end_middle_right\", \"end_defensive_left\", \"end_defensive_right\",\n",
    "    \"end_attacking_center\", \"end_defensive_center\", \"end_middle_center\",\n",
    "    'all_passes', 'successful_passes','all_shots', 'all_dribbles',\n",
    "    \n",
    "    'pass_success_rate','extra_shots','first_time_shots','shots_on_target','assists','goals','player_id', 'successful_dribbles_count','successful_dribbles','defensive_start', 'middle_start', 'attacking_start'\n",
    "    ,'Carry', 'shot_mean_length','avg_pass_length','median_carry_end', 'total_packing', 'total_width_change'\n",
    "    # Highly Correlated Features\n",
    "    #\"ground_pass\",\"total_width_change\", \"successful_passes\", \"pressure_count\",\"all_passes\", \"total_distance\",\n",
    "    #\"total_width_change\", \"total_packing\", \"total_duration\", \"wide_focus\", \"defensive_intensity\", \"on_ball_actions\", \"defensive_actions\"\n",
    "    # 'avg_directness','low_pass', 'Shot',\n",
    "    #   'pass_switch', 'pass_cut_back', 'aerial_duels_won', 'defensive_start', 'middle_start'\n",
    "]\n",
    "player_features_dr = player_features.drop(columns=features_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_numeric_cols = ['player_id', 'player_name', 'minutes']\n",
    "\n",
    "df_numeric = player_features_dr.drop(columns=non_numeric_cols, errors='ignore')\n",
    "corr_matrix = df_numeric.corr()\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap=\"coolwarm\", center=0, linewidths=0.5, vmin=-1, vmax=1)\n",
    "plt.title(\"Feature Correlation Heatmap\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "corr_matrix = df_numeric.corr().abs()\n",
    "upper_triangle = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "high_corr_pairs = [\n",
    "    (col, row, upper_triangle.loc[row, col])\n",
    "    for col in upper_triangle.columns\n",
    "    for row in upper_triangle.index\n",
    "    if upper_triangle.loc[row, col] > 0.80\n",
    "]\n",
    "\n",
    "if high_corr_pairs:\n",
    "    print(\"Highly correlated feature pairs (correlation > 0.80):\")\n",
    "    for col1, col2, corr_value in high_corr_pairs:\n",
    "        print(f\"{col1} and {col2}: {corr_value:.2f}\")\n",
    "else:\n",
    "    print(\"No feature pairs with correlation greater than 0.80.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "latent_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for hm in heatmaps:\n",
    "        x = torch.tensor(hm, dtype=torch.float32).unsqueeze(0).unsqueeze(0)  # [1,1,6,4]\n",
    "        z = model.encoder(x.view(1, -1))  # [1, latent_dim]\n",
    "        latent_list.append(z.squeeze(0).numpy())\n",
    "\n",
    "latent_array = np.vstack(latent_list)  # shape: [N, latent_dim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_df = pd.DataFrame(latent_array, columns=[f\"latent_{i}\" for i in range(latent_array.shape[1])])\n",
    "\n",
    "df_numeric = pd.concat([df_numeric.reset_index(drop=True), latent_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "position_columns = ['Goalkeeper','Center Back', 'Wide Back', 'Center Midfield', 'Wide Midfield', 'Striker']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = df_numeric.corr()\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap=\"coolwarm\", center=0, linewidths=0.5, vmin=-1, vmax=1)\n",
    "plt.title(\"Feature Correlation Heatmap\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closure(arr):\n",
    "    \"\"\"L1-normalize rows so they sum to 1 (inplace-safe).\"\"\"\n",
    "    arr = np.asarray(arr, dtype=float)\n",
    "    row_sums = arr.sum(axis=1, keepdims=True)\n",
    "    # avoid division by zero\n",
    "    row_sums[row_sums == 0] = 1.0\n",
    "    return arr / row_sums\n",
    "\n",
    "def clr_transform(arr, pseudocount=1e-6):\n",
    "    \"\"\"Apply CLR row-wise. arr must be positive. Returns array of same shape.\"\"\"\n",
    "    arr = np.asarray(arr, dtype=float) + pseudocount\n",
    "    logA = np.log(arr)\n",
    "    gm = logA.mean(axis=1, keepdims=True)\n",
    "    return logA - gm\n",
    "\n",
    "def centered_log_ratio(player_df):\n",
    "    part_cols = [c for c in player_df.columns if c.startswith(\"latent_\")]\n",
    "    stat_cols = [c for c in player_df.columns if not c.startswith(\"latent_\") and c not in position_columns]\n",
    "    pos_comp = closure(player_df[position_columns].values)\n",
    "\n",
    "    pos_clr = clr_transform(pos_comp, pseudocount=1e-6)\n",
    "    scaler = StandardScaler()\n",
    "    pos_clr = scaler.fit_transform(pos_clr)\n",
    "    part_scaled = scaler.fit_transform(player_df[part_cols])\n",
    "    scaler2 = StandardScaler()\n",
    "    stat_scaled = scaler2.fit_transform(player_df[stat_cols])\n",
    "    # optional: weight groups (example: make positions twice as important)\n",
    "    pos_weight = 2\n",
    "    part_weight = 1.8\n",
    "    stat_weight = 0.8\n",
    "    part_scaled = np.clip(part_scaled, -3, 3)\n",
    "    stat_scaled = np.clip(stat_scaled, -3, 3)\n",
    "    pos_clr *= pos_weight\n",
    "    part_scaled *= part_weight\n",
    "    stat_scaled *= stat_weight\n",
    "\n",
    "    #print(stat_cols)\n",
    "    #print(stat_scaled)\n",
    "    # --- concat, scale, cluster ---\n",
    "    X = np.hstack([pos_clr, part_scaled, stat_scaled])\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "scaled_features = centered_log_ratio(df_numeric)\n",
    "part_cols = [c for c in df_numeric.columns if c.startswith(\"latent_\")]\n",
    "stat_cols = [c for c in df_numeric.columns if not c.startswith(\"latent_\") and c not in position_columns]\n",
    "correct_order = position_columns + part_cols + stat_cols\n",
    "\n",
    "pca_df = pd.DataFrame(scaled_features, columns=correct_order)\n",
    "pd.set_option('display.max_rows', None)\n",
    "#print(pca_df.std())\n",
    "pd.reset_option('display.max_rows')\n",
    "\n",
    "position_indices = [df_numeric.columns.get_loc(c) for c in position_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the optimal number of clusters using the Elbow Method\n",
    "wcss = []  # Within-cluster sum of squares\n",
    "\n",
    "max_cluster_test = 10\n",
    "\n",
    "for i in range(2, max_cluster_test):  \n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "    kmeans.fit(pca_df)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plotting the Elbow Curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(2, max_cluster_test), wcss, marker='o', linestyle='--')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Silhouette Score to support the elbow method\n",
    "silhouette_scores = []\n",
    "for n_clusters in range(2, max_cluster_test):\n",
    "    kmeans = KMeans(\n",
    "    n_clusters=n_clusters,\n",
    "    init='k-means++',  \n",
    "    n_init=50,\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    "    )\n",
    "    cluster_labels = kmeans.fit_predict(pca_df)\n",
    "    silhouette_avg = silhouette_score(pca_df, cluster_labels)\n",
    "    silhouette_scores.append(silhouette_avg)\n",
    "\n",
    "# Plot Silhouette Scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(2, max_cluster_test), silhouette_scores, marker='s', linestyle='-', color='orange')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for Optimal Clusters')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Apply K-Means with the optimal number of clusters (let's assume 4 based on elbow/silhouette)\n",
    "optimal_clusters = 6  # Adjust this based on your elbow/silhouette plot results\n",
    "kmeans_final = KMeans(\n",
    "    n_clusters=optimal_clusters,\n",
    "    init='k-means++',  \n",
    "    n_init=50,\n",
    "    max_iter=500,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#pca_df = pd.DataFrame(scaled_features, columns=df_numeric.columns)\n",
    "\n",
    "pca_df.to_csv('../../excel/pca_df.csv', index=False)\n",
    "#pca_df = df_numeric.copy()\n",
    "# Predict clusters\n",
    "pca_df['Cluster'] = kmeans_final.fit_predict(pca_df)\n",
    "\n",
    "\n",
    "\n",
    "# Analyze the average feature values per cluster (to profile playing styles)\n",
    "cluster_profiles = pd.concat([pca_df, df_numeric.reset_index(drop=True)], axis=1)\n",
    "cluster_summary = cluster_profiles.groupby('Cluster').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_features[\"Cluster\"] = pca_df['Cluster']\n",
    "\n",
    "player_features.columns\n",
    "\n",
    "#assert (player_features.index == pca_df.index).all(), \"Row order changed!\"\n",
    "\n",
    "names_unique = df_event.loc[:,['player_id']].drop_duplicates(subset=[\"player_id\"]).reset_index()\n",
    "player_features_with_name = player_features.merge(names_unique, on=\"player_id\", how=\"left\")\n",
    "df_player = pca_df.copy()\n",
    "df_player[\"player_name\"] = player_features_with_name[\"player_name\"] \n",
    "#df_player.to_csv('../../excel/pca_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_means = player_features.groupby('Cluster')[position_columns].mean()\n",
    "cluster_to_position = cluster_means.idxmax(axis=1).to_dict()\n",
    "\n",
    "player_features_with_name['Cluster'] = player_features_with_name['Cluster'].map(cluster_to_position)\n",
    "\n",
    "cluster_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column for subclusters\n",
    "pca_df['SubCluster'] = -1  # placeholder\n",
    "\n",
    "all_centers = []\n",
    "\n",
    "for cluster_id in np.sort(pca_df['Cluster'].unique()):\n",
    "    # Select data for this cluster\n",
    "    cluster_data = pca_df[pca_df['Cluster'] == cluster_id].drop(columns=['Cluster', 'SubCluster'])\n",
    "    cluster_data = cluster_data.drop(columns=position_columns)\n",
    "    #if cluster_id == 5:\n",
    "    #    cluster_data = cluster_data.loc[:, [\"Pass\",\"Carry\",\"avg_pass_length\", \"carry_length_forward\", \"median_carry_start\", \"avg_progression\", \"avg_directness\"]]\n",
    "    scaler = StandardScaler()\n",
    "    cluster_data = pd.DataFrame(scaler.fit_transform(cluster_data), columns=cluster_data.columns)\n",
    "\n",
    "    # Decide how many subclusters you want\n",
    "    print(cluster_data.shape)\n",
    "    n_subclusters = n_subclusters = min(3, len(cluster_data)//30)  # or choose dynamically based on len(cluster_data)\n",
    "\n",
    "    print(f'{n_subclusters} for cluster {cluster_to_position[cluster_id]}')\n",
    "    \n",
    "    # Only cluster if there are enough points\n",
    "    if len(cluster_data) > n_subclusters:\n",
    "        sub_kmeans = KMeans(\n",
    "            n_clusters=n_subclusters,\n",
    "            init='k-means++',  \n",
    "            n_init=50,\n",
    "            max_iter=500,\n",
    "            random_state=42\n",
    "        )\n",
    "        pca_df.loc[pca_df['Cluster'] == cluster_id, 'SubCluster'] = sub_kmeans.fit_predict(cluster_data)\n",
    "        centers = pd.DataFrame(\n",
    "            sub_kmeans.cluster_centers_,\n",
    "            columns=cluster_data.columns\n",
    "        )\n",
    "        all_centers.append(centers)\n",
    "        centers.to_csv(f'../../excel/centers_{cluster_to_position[cluster_id]}.csv')\n",
    "\n",
    "        feature_variance = centers.var(axis=0)\n",
    "\n",
    "        # Normalize to sum to 1 to get relative importance\n",
    "        feature_importance = feature_variance / feature_variance.sum()\n",
    "\n",
    "        # Create a DataFrame for display\n",
    "        importance_df = pd.DataFrame({\n",
    "            'Feature': feature_importance.index,\n",
    "            'Importance': feature_importance.values\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        top_features = importance_df[~importance_df[\"Feature\"].str.contains(\"latent_\")].head(20)\n",
    "        print(top_features)\n",
    "        top_feature_names = top_features[\"Feature\"].tolist()\n",
    "        centers_long = centers.melt(ignore_index=False, var_name=\"Feature\", value_name=\"Value\")\n",
    "        centers_long = centers_long.reset_index().rename(columns={\"index\": \"Cluster\"})\n",
    "\n",
    "        \n",
    "        centers_top10 = centers[top_feature_names]\n",
    "        \n",
    "        plt.figure(figsize=(20, 6))\n",
    "        sns.barplot(data=centers_long, x=\"Feature\", y=\"Value\", hue=\"Cluster\")\n",
    "\n",
    "        plt.title(\"Cluster Centers — Feature Importance (Normalized)\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        plt.figure(figsize=(16, 6))\n",
    "        sns.heatmap(\n",
    "            centers_top10,\n",
    "            cmap=\"viridis\",       # or any colormap you like\n",
    "            annot=True,           # show numeric values\n",
    "            fmt=\".2f\",            # format annotation\n",
    "            linewidths=0.5,       # grid lines between cells\n",
    "            cbar_kws={\"label\": \"Cluster center value\"}  # colorbar label\n",
    "        )\n",
    "\n",
    "        plt.title(\"Cluster/Subcluster Centers — Feature Values\")\n",
    "        plt.xlabel(\"Features\")\n",
    "        plt.ylabel(\"SubCluster\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "\n",
    "    else:\n",
    "        # If too few points, assign all to one subcluster (e.g., 0)\n",
    "        pca_df.loc[pca_df['Cluster'] == cluster_id, 'SubCluster'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_features[\"SubCluster\"] = pca_df['SubCluster']\n",
    "player_features_with_name[\"SubCluster\"] = pca_df['SubCluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_features_with_name.groupby([\"Cluster\", \"SubCluster\"]).agg(\n",
    "    minutes_mean = (\"minutes\", \"mean\"),\n",
    "    minutes_median = (\"minutes\", \"median\"),\n",
    "    minutes_sum  = (\"minutes\", \"sum\"),\n",
    "    count        = (\"player_id\", \"count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import Session\n",
    "from sqlalchemy import delete\n",
    "from sql_schemas import PlayerStats\n",
    "\n",
    "\n",
    "def init_player_stats(db: Session, df, tableClass=PlayerStats):\n",
    "    # 1️⃣ Clear existing table\n",
    "    db.execute(delete(tableClass))\n",
    "    db.commit()\n",
    "\n",
    "    # 2️⃣ Rename DataFrame columns to match ORM (adjust names if needed)\n",
    "    df = df.rename(columns={\n",
    "        \"player_id\": \"id\",\n",
    "        \"Pass\": \"pass_90\",\n",
    "        \"Carry\": \"carry_90\",\n",
    "        \"Dribble\": \"dribble_90\",\n",
    "        \"Shot\": \"shot_90\",\n",
    "        \"Duel\": \"duel_90\",\n",
    "        \"Pressure\": \"pressure_90\",\n",
    "        \"Block\": \"block_90\",\n",
    "        \"Ball Recovery\": \"ball_recovery_90\",\n",
    "        \"Interception\": \"interception_90\",\n",
    "        \"Clearance\": \"clearance_90\",\n",
    "        \"Foul Won\": \"foul_won_90\",\n",
    "        \"Foul Committed\": \"foul_commited_90\",\n",
    "        \"shot_mean_length\": \"average_shot_length\",\n",
    "        \"headers\": \"header_percent\",\n",
    "        \"total_xg\": \"xg_90\",\n",
    "        \"successful_dribbles\": \"dribble_percent\",\n",
    "        \"dribble_start_x\": \"average_dribble_start_x\",\n",
    "        \"ground_pass\": \"ground_pass_percent\",\n",
    "        \"low_pass\": \"low_pass_percent\",\n",
    "        \"high_pass\": \"high_pass_percent\",\n",
    "        \"Goalkeeper\": \"goalkeeper\",\n",
    "        \"Center Back\": \"center_back\",\n",
    "        \"Wide Back\": \"wide_back\",\n",
    "        \"Center Midfield\": \"center_midfielder\",\n",
    "        \"Wide Midfield\": \"wide_midfielder\",\n",
    "        \"Striker\": \"striker\",\n",
    "        \"defensive_start\": \"defensive_start_percent\",\n",
    "        \"middle_start\": \"middle_start_percent\",\n",
    "        \"attacking_start\": \"attacking_start_percent\",\n",
    "        \"pass_success_rate\":\"pass_success_rate_percent\",\n",
    "        \"extra_shots\":\"extra_shots_percent\",\n",
    "        \"first_time_shots\":\"first_time_shots_percent\" ,\n",
    "        \"shots_on_target\":\"shots_on_target_percent\",\n",
    "        \"assists\":\"assists_90\",\n",
    "        \"goals\":\"goals_90\",\n",
    "        \"SubCluster\" : \"cluster\"\n",
    "    })\n",
    "\n",
    "    orm_columns = set(c.name for c in tableClass.__table__.columns)\n",
    "\n",
    "    df_filtered = df[[col for col in df.columns if col in orm_columns]]\n",
    "\n",
    "    new_rows = [\n",
    "        tableClass(**p)\n",
    "        for p in df_filtered.to_dict(orient=\"records\")\n",
    "    ]\n",
    "    db.bulk_save_objects(new_rows)\n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_features_with_name.to_excel('../../excel/player_features_results.xlsx', index=False)\n",
    "init_player_stats(db, player_features_with_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts = player_features_with_name[\"SubCluster\"].value_counts().sort_index()\n",
    "print(cluster_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_features_with_name.loc[:, [\"Pass\"]].describe()\n",
    "player_features_with_name[player_features_with_name[\"Shot\"]==0]['minutes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {\n",
    "    ('Goalkeeper', 0): 'Line Keeper',\n",
    "    ('Goalkeeper', 1): 'Ball Playing Keeper',\n",
    "    ('Center Back', 0): 'Sweeper',\n",
    "    ('Center Back', 1): 'Stopper',\n",
    "    ('Center Back', 2): 'Ball Playing Defender',\n",
    "    ('Center Midfield', 0): 'Box To Box Midfielder',\n",
    "    ('Center Midfield', 1): 'Advanced Playmaker',\n",
    "    ('Center Midfield', 2): 'Holding Midfielder',\n",
    "    ('Wide Back', 0): 'Inverted Full Back',\n",
    "    ('Wide Back', 1): 'Wing Back',\n",
    "    ('Wide Back', 2): 'Full Back',\n",
    "    ('Wide Midfield', 0): 'Winger',\n",
    "    ('Wide Midfield', 1): 'Inside Forward',\n",
    "    ('Wide Midfield', 2): 'Wide Playmaker',\n",
    "    ('Striker', 0): 'Target Man',\n",
    "    ('Striker', 1): 'Mobile Striker',\n",
    "    ('Striker', 2): 'Second Striker'\n",
    "}\n",
    "#player_features_with_name['SubCluster'] = player_features_with_name.apply(lambda row: mapping.get((row['Cluster'], row['SubCluster'])), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = pd.DataFrame(\n",
    "    kmeans_final.cluster_centers_,\n",
    "    columns=correct_order\n",
    ")\n",
    "centers_long = centers.melt(ignore_index=False, var_name=\"Feature\", value_name=\"Value\")\n",
    "centers_long = centers_long.reset_index().rename(columns={\"index\": \"Cluster\"})\n",
    "\n",
    "plt.figure(figsize=(20, 6))\n",
    "sns.barplot(data=centers_long, x=\"Feature\", y=\"Value\", hue=\"Cluster\")\n",
    "\n",
    "plt.title(\"Cluster Centers — Feature Importance (Normalized)\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_numeric_cluster = df_numeric.copy()\n",
    "df_numeric_cluster[\"Cluster\"] = pca_df[\"Cluster\"]\n",
    "cluster_centers_original = (\n",
    "    df_numeric_cluster.groupby(\"Cluster\")[[x for x in pca_df.columns if \"Cluster\" not in x]]\n",
    "      .mean()\n",
    "      .reset_index()\n",
    ")\n",
    "\n",
    "print(cluster_centers_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_cluster(j):\n",
    "    for k in player_features_with_name[\"SubCluster\"].unique():\n",
    "        cluster_ids = player_features_with_name.loc[\n",
    "            (player_features_with_name[\"Cluster\"] == j) & (player_features_with_name[\"SubCluster\"] == k), \n",
    "            \"player_id\"\n",
    "        ].unique()\n",
    "        if len(cluster_ids)>10:\n",
    "            filtered_ids = [i for i in ids if i in cluster_ids]\n",
    "            filtered_heatmaps = [\n",
    "                heatmap for heatmap, pid in zip(heatmaps, ids) if pid in cluster_ids\n",
    "            ]\n",
    "            print(f\"{j},{k}\")\n",
    "            plot = visualize_players(filtered_heatmaps, filtered_ids, \"ret\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_features_list = [\"Pass\", \"pass_median_start\",\"pass_median_width_start\",\"high_pass\",\"median_pass_length\", \"Carry\", \"Interception\",\"counterpress_count\", \"avg_directness\"] \n",
    "grouped = player_features_with_name.loc[:, highlight_features_list + [\"Cluster\", \"SubCluster\"]].groupby([\"Cluster\", \"SubCluster\"])\n",
    "\n",
    "# Select only numeric feature columns\n",
    "numeric_cols = player_features_with_name.select_dtypes(include='number').columns\n",
    "\n",
    "stats_df = (\n",
    "    player_features_with_name\n",
    "        .groupby([\"Cluster\", \"SubCluster\"])[highlight_features_list]\n",
    "        .agg(['mean'])\n",
    ")\n",
    "\n",
    "# Flatten multi-index column names\n",
    "stats_df.columns = ['_'.join(col) for col in stats_df.columns]\n",
    "\n",
    "# Save to CSV\n",
    "stats_df.to_csv(\"cluster_subcluster_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in player_features_with_name[\"Cluster\"].unique():\n",
    "    for k in player_features_with_name[\"SubCluster\"].unique():\n",
    "        cluster_ids = player_features_with_name.loc[\n",
    "            (player_features_with_name[\"Cluster\"] == j) & (player_features_with_name[\"SubCluster\"] == k), \n",
    "            \"player_id\"\n",
    "        ].unique()\n",
    "        if len(cluster_ids)>10:\n",
    "            filtered_ids = [i for i in ids if i in cluster_ids]\n",
    "            filtered_heatmaps = [\n",
    "                heatmap for heatmap, pid in zip(heatmaps, ids) if pid in cluster_ids\n",
    "            ]\n",
    "            print(f\"{j},{k}\")\n",
    "\n",
    "            visualize_players(filtered_heatmaps, filtered_ids, f\"../../plots/players_cluster_{j}_{k}.png\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = player_features_with_name.loc[:,[\"Cluster\", 'avg_pass_length', 'wide_focus', 'attacking_start','total_xg']].groupby([\"Cluster\"]).describe().T\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_features['Cluster'] = pca_df['Cluster']\n",
    "df_event_with_clusters = df_event.merge(\n",
    "    player_features[['player_id', 'Cluster']],\n",
    "    on=['player_id'],\n",
    "    how='left')\n",
    "df_event_with_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def visualize_players2(heatmaps, ids):\n",
    "    num_players = 5\n",
    "    rows, cols = 1, 5\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(10, 4))\n",
    "\n",
    "    for i in range(num_players):\n",
    "        r, c = divmod(i, cols)\n",
    "        ax = axes[c]\n",
    "\n",
    "        ax.imshow(heatmaps[i], cmap='Greens', origin='lower')\n",
    "\n",
    "        # Title\n",
    "        match = player_name[player_name[\"player_id\"] == ids[i]]\n",
    "        name = match.iloc[0].player_name if not match.empty else \"Unknown\"\n",
    "        ax.set_title(name)\n",
    "\n",
    "        #ax.axis(\"off\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # --- Convert figure to image array ---\n",
    "    fig.canvas.draw()\n",
    "    img = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8)\n",
    "    img = img.reshape(fig.canvas.get_width_height()[::-1] + (4,))\n",
    "\n",
    "    plt.close(fig)  # avoid drawing\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_title(title, max_length=20):\n",
    "    if len(title) > max_length:\n",
    "        # Find indices of all spaces\n",
    "        space_indices = [i for i, c in enumerate(title) if c == \" \"]\n",
    "        if len(space_indices) >= 2:\n",
    "            split_idx = space_indices[1]\n",
    "            title = title[:split_idx] + \"\\n\" + title[split_idx+1:]\n",
    "        else:\n",
    "            # Fallback: break at first space\n",
    "            title = title.replace(\" \", \"\\n\", 1)\n",
    "    return title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in player_features_with_name[\"Cluster\"].unique():\n",
    "\n",
    "    images = []\n",
    "    titles = []\n",
    "\n",
    "    for k in player_features_with_name[\"SubCluster\"].unique():\n",
    "\n",
    "        cluster_ids = player_features_with_name.loc[\n",
    "            (player_features_with_name[\"Cluster\"] == j) &\n",
    "            (player_features_with_name[\"SubCluster\"] == k)\n",
    "        ].sort_values(by=\"minutes\", ascending=False).head(10)[\"player_id\"].unique()\n",
    "\n",
    "        if len(cluster_ids) <= 10:\n",
    "            continue\n",
    "\n",
    "        filtered_heatmaps = [\n",
    "            hm for hm, pid in zip(heatmaps, ids) if pid in cluster_ids\n",
    "        ]\n",
    "        filtered_ids = [pid for pid in ids if pid in cluster_ids]\n",
    "\n",
    "        img = visualize_players2(filtered_heatmaps, filtered_ids)\n",
    "\n",
    "        images.append(img)\n",
    "        titles.append(f\"SubCluster {k}\")\n",
    "\n",
    "    if not images:\n",
    "        continue\n",
    "\n",
    "    # ---- Build final stacked plot ----\n",
    "    n = len(images)\n",
    "    fig, axes = plt.subplots(n, 1, figsize=(10, 4*n))\n",
    "\n",
    "    if n == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    for ax, img, title in zip(axes, images, titles):\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(format_title(title))\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"../../plots/cluster_{j}_combined.png\", dpi=150)\n",
    "    plt.close(fig)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "football-scouting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
